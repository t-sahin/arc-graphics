#include "DataTypes.cginc"


#pragma kernel ReduceSum1
#pragma kernel ReduceSum2
#pragma kernel PrepareModifierSum

//Max is 512 based on code below!
#define REDUCTION_BLOCKSIZE 128

RWStructuredBuffer<int3> tempSum; //used to go between sums. Should equal blocksize number
StructuredBuffer<ParticleProperties> properties;
RWStructuredBuffer<int3> inputSum;
RWStructuredBuffer<int3> result;


uint2 modifier;
uint dispatchDim;



groupshared int3 sdata[REDUCTION_BLOCKSIZE];

//The code below will reduce the sum from N down to the number called on dispatch
//Make sure that the tempSum buffer is larger/equal to the dispatch number
//If you call with REDUCTION_BLOCKSIZE < 2 N, you will have an error. Make sure to set dispatchDim between
//dispatches if your input buffer is changing sizes
//Also it requires that n is an integer multiple of 2 * REDUCTION_BLOCKSIZE

//TL;DR
//setUint(dispatchDim = REDUCTION_BLOCKSIZE) -> this means only two sums are required
//dispatch(dispatchDim, sum1)
//dispatch(1, sum2)

//from reading docs, it seems like tid should be the larger number.
//This code makes it seem like the smaller index
//also the names suggest that. I'm going to go with that
[numthreads(REDUCTION_BLOCKSIZE, 1, 1)]
void ReduceSum1(uint tid : SV_GroupIndex, uint3 groupIdx : SV_GroupID)
{
	uint n, _;
	inputSum.GetDimensions(n, _);

	//get the offset for where we will read
	//notice that within groups, they should have neighboring is.
	unsigned int i = groupIdx.x*(REDUCTION_BLOCKSIZE * 2) + tid;
	unsigned int dispatchSize = REDUCTION_BLOCKSIZE * 2 * dispatchDim;

	sdata[tid] = 0; //this data is shared within the group, so tid < REDUCTION_BLOCKSIZE

	do {
		//we read our assigned value AND one from a neighboring value.
		//This extra value is why we have a factor of 2 above
		sdata[tid] += inputSum[i] + inputSum[i + REDUCTION_BLOCKSIZE];
		i += dispatchSize; //we increment to the next set if we need to based on the size of the input array (n)
	} while (i < n);

	GroupMemoryBarrierWithGroupSync(); //wait for all threads in group to finish reading

									   //based on the size of the thread groups (which is fixed at compile time)
									   //we perform one extra left-hand side reduction. So tid 4, for example, will grab what tid 68 had
									   //all sums are now stored between 0 and 64
	if (REDUCTION_BLOCKSIZE >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } GroupMemoryBarrierWithGroupSync(); }

	if (REDUCTION_BLOCKSIZE >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } GroupMemoryBarrierWithGroupSync(); }

	if (REDUCTION_BLOCKSIZE >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } GroupMemoryBarrierWithGroupSync(); }

	//why do the above require sync, but not those below?
	//becaue each step above requires data from the previous step

	//why do the below not require sync? Because Instructions are SIMD synchronous within a warp
	//at < 32, we are in a single warp.

	if (tid < 32) {
		if (REDUCTION_BLOCKSIZE >= 64) sdata[tid] += sdata[tid + 32]; //now all sums are between 0 and 32
		if (REDUCTION_BLOCKSIZE >= 32) sdata[tid] += sdata[tid + 16]; //between 0 and 16....
		if (REDUCTION_BLOCKSIZE >= 16) sdata[tid] += sdata[tid + 8];
		if (REDUCTION_BLOCKSIZE >= 8) sdata[tid] += sdata[tid + 4];
		if (REDUCTION_BLOCKSIZE >= 4) sdata[tid] += sdata[tid + 2];
		if (REDUCTION_BLOCKSIZE >= 2) sdata[tid] += sdata[tid + 1];
	}

	if (tid == 0) {
		tempSum[groupIdx.x] = sdata[0]; //finally we are done in our group and add that to output data.
	}

}

//same as above, but requires n == groupDim and does not use two elments for summation.
[numthreads(REDUCTION_BLOCKSIZE, 1, 1)]
void ReduceSum2(uint tid : SV_GroupIndex, uint3 groupIdx : SV_GroupID)
{
	//get the offset for where we will read
	//notice that within groups, they should have neighboring is.
	unsigned int i = groupIdx.x * REDUCTION_BLOCKSIZE + tid;

	sdata[tid] = 0; //this data is shared within the group, so tid < REDUCTION_BLOCKSIZE


	sdata[tid] += tempSum[i]; //got rid of the extra summation term to better balance sum1/sum2

	GroupMemoryBarrierWithGroupSync(); //wait for all threads in group to finish reading

									   //based on the size of the thread groups (which is fixed at compile time)
									   //we perform one extra left-hand side reduction. So tid 4, for example, will grab what tid 68 had
									   //all sums are now stored between 0 and 64
	if (REDUCTION_BLOCKSIZE >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } GroupMemoryBarrierWithGroupSync(); }

	if (REDUCTION_BLOCKSIZE >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } GroupMemoryBarrierWithGroupSync(); }

	if (REDUCTION_BLOCKSIZE >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } GroupMemoryBarrierWithGroupSync(); }

	if (tid < 32) {
		if (REDUCTION_BLOCKSIZE >= 64) sdata[tid] += sdata[tid + 32]; //now all sums are between 0 and 32
		if (REDUCTION_BLOCKSIZE >= 32) sdata[tid] += sdata[tid + 16]; //between 0 and 16....
		if (REDUCTION_BLOCKSIZE >= 16) sdata[tid] += sdata[tid + 8];
		if (REDUCTION_BLOCKSIZE >= 8) sdata[tid] += sdata[tid + 4];
		if (REDUCTION_BLOCKSIZE >= 4) sdata[tid] += sdata[tid + 2];
		if (REDUCTION_BLOCKSIZE >= 2) sdata[tid] += sdata[tid + 1];
	}

	if (tid == 0) {
		result[0] = sdata[0];
		//result[0] = int3(100, 100, 100);
	}

}

[numthreads(PARTICLE_BLOCKSIZE, 1, 1)]
void PrepareModifierSum(uint3 id : SV_DispatchThreadID) {
	uint state;
	inputSum[id.x] = int3(0, 0, 0);
	if (properties[id.x].lastModifier == modifier[0] && properties[id.x].lastModifierIndex == modifier[1]) {
		state = properties[id.x].state;
		inputSum[id.x] = int3(state == PARTICLE_STATE_DEAD, state == PARTICLE_STATE_ALIVE, state > 1);
	}
}